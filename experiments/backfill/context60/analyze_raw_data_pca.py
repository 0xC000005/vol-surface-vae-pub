#!/usr/bin/env python3
"""
Raw Data PCA Analysis - Volatility Surface Intrinsic Dimensionality

This script analyzes the raw volatility surface data (before VAE compression)
to determine how many dimensions are fundamentally needed to represent the data.

If raw data requires only 3-4 components for 90% variance, then the VAE using
3/12 dimensions is CORRECTLY learning the true structure (SUCCESS).

If raw data requires 10+ components, then the VAE is over-compressing (FAILURE).
"""

import numpy as np
from pathlib import Path
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Setup paths
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
DATA_PATH = PROJECT_ROOT / "data" / "vol_surface_with_ret.npz"
OUTPUT_DIR = PROJECT_ROOT / "results" / "context60_baseline" / "analysis_v2" / "raw_data_pca"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

print("=" * 80)
print("RAW DATA PCA ANALYSIS - INTRINSIC DIMENSIONALITY")
print("=" * 80)
print()
print("Question: Is 3 dimensions fundamental to volatility surfaces?")
print()
print("If raw data uses 3-4 dimensions for 90% variance:")
print("  âœ… VAE latent12 V2 using 3/12 dims = SUCCESS (learning true structure)")
print()
print("If raw data uses 10+ dimensions for 90% variance:")
print("  âŒ VAE latent12 V2 using 3/12 dims = FAILURE (over-compressing)")
print()
print("=" * 80)
print()

# Load data
print("Loading raw volatility surface data...")
data = np.load(DATA_PATH)
surfaces = data["surface"]  # (N, 5, 5)
print(f"Data shape: {surfaces.shape}")
print(f"Total days: {len(surfaces)}")
print()

# Use same test set as model evaluation (last 20%)
train_size = int(0.8 * len(surfaces))
test_surfaces = surfaces[train_size:]
print(f"Using test set: {len(test_surfaces)} days (last 20%)")
print()

# Flatten surfaces to 2D for PCA
# Each day: 5Ã—5 grid â†’ 25-dimensional vector
surfaces_flat = test_surfaces.reshape(len(test_surfaces), -1)  # (N, 25)
print(f"Flattened shape: {surfaces_flat.shape}")
print()

# Run PCA
print("Running PCA on raw data...")
pca = PCA()
pca.fit(surfaces_flat)

explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# How many components for 90% variance?
n_90 = np.argmax(cumulative_variance >= 0.9) + 1
n_95 = np.argmax(cumulative_variance >= 0.95) + 1
n_99 = np.argmax(cumulative_variance >= 0.99) + 1

print("=" * 80)
print("RESULTS: RAW DATA PCA")
print("=" * 80)
print()
print(f"Total dimensions: {surfaces_flat.shape[1]} (5Ã—5 grid)")
print()
print("Variance explained by top components:")
for i in range(min(10, len(explained_variance_ratio))):
    print(f"  PC{i+1}: {explained_variance_ratio[i] * 100:.2f}% (cumulative: {cumulative_variance[i] * 100:.2f}%)")
print()
print(f"Components needed for 90% variance: {n_90}")
print(f"Components needed for 95% variance: {n_95}")
print(f"Components needed for 99% variance: {n_99}")
print()

# Interpretation
print("=" * 80)
print("INTERPRETATION")
print("=" * 80)
print()

if n_90 <= 4:
    print("âœ… VOLATILITY SURFACES ARE LOW-DIMENSIONAL (â‰¤4 components for 90%)")
    print()
    print("FINDING: Volatility surfaces are inherently 3-4 dimensional.")
    print()
    print("Expected fundamental factors:")
    print(f"  - PC1 ({explained_variance_ratio[0] * 100:.1f}%): Overall volatility LEVEL")
    print(f"  - PC2 ({explained_variance_ratio[1] * 100:.1f}%): Term structure SLOPE")
    print(f"  - PC3 ({explained_variance_ratio[2] * 100:.1f}%): Skew/smile CURVATURE")
    if n_90 == 4:
        print(f"  - PC4 ({explained_variance_ratio[3] * 100:.1f}%): Higher-order structure")
    print()
    print("IMPLICATION FOR VAE LATENT12 V2:")
    print("  âœ… Using 3/12 dimensions = SUCCESS")
    print("  âœ… Model is correctly learning the true 3-factor structure")
    print("  âœ… Additional 9 latent dims provide flexibility without over-compressing")
    print()
    print("VAE Results (latent12 V2):")
    print("  - PC1: 68.05% (level)")
    print("  - PC2: 15.78% (slope)")
    print("  - PC3: 9.63% (curvature)")
    print()
    print("COMPARISON:")
    print(f"  Raw data PC1: {explained_variance_ratio[0] * 100:.1f}%")
    print(f"  VAE latent PC1: 68.05%")
    print(f"  â†’ VAE is LESS compressed than raw data (good regularization)")
    print()
    print("CONCLUSION:")
    print("  ðŸŽ‰ 3 dimensions is OPTIMAL for volatility surfaces")
    print("  ðŸŽ‰ VAE latent12 V2 is learning the correct structure")
    print("  ðŸŽ‰ The question 'why only 3 dimensions?' is answered:")
    print("     Because volatility surfaces ARE 3-dimensional!")
    print()
    print("NEXT STEPS:")
    print("  - Continue training to completion (27 hours remaining)")
    print("  - Focus on improving correlation (0.130 â†’ 0.35+)")
    print("  - Investigate decoder bottleneck (not latent bottleneck)")

elif n_90 <= 7:
    print("âš ï¸  VOLATILITY SURFACES ARE MODERATELY DIMENSIONAL (5-7 components)")
    print()
    print(f"FINDING: Raw data needs {n_90} components for 90% variance.")
    print()
    print("IMPLICATION FOR VAE LATENT12 V2:")
    print("  âš ï¸  Using 3/12 dimensions may be slightly under-utilizing capacity")
    print("  âš ï¸  Could potentially use 4-5 dimensions with weaker KL weight")
    print()
    print("RECOMMENDED ACTION:")
    print("  1. Wait for training to complete (may improve to 4-5 dims)")
    print("  2. If still 3 dims, consider kl_weight=5e-6 (2Ã— weaker)")
    print("  3. Test if correlation improves with more dimensions used")

else:
    print("âŒ VOLATILITY SURFACES ARE HIGH-DIMENSIONAL (8+ components)")
    print()
    print(f"FINDING: Raw data needs {n_90} components for 90% variance.")
    print()
    print("IMPLICATION FOR VAE LATENT12 V2:")
    print("  âŒ Using only 3/12 dimensions = SEVERE UNDER-UTILIZATION")
    print("  âŒ KL weight is still too strong (over-regularizing)")
    print()
    print("RECOMMENDED ACTION:")
    print("  1. Reduce kl_weight: 1e-5 â†’ 5e-6 (2Ã— weaker)")
    print("  2. Expected: Use 5-7 dimensions after retraining")
    print("  3. May improve correlation significantly")

print()

# Visualization 1: Scree plot
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left: Individual variance
ax = axes[0]
n_components_to_plot = min(15, len(explained_variance_ratio))
ax.bar(np.arange(n_components_to_plot) + 1, explained_variance_ratio[:n_components_to_plot] * 100, alpha=0.7)
ax.axhline(10, color='red', linestyle='--', linewidth=1, alpha=0.5, label='10% threshold')
ax.set_xlabel('Principal Component', fontsize=12)
ax.set_ylabel('Explained Variance (%)', fontsize=12)
ax.set_title('Raw Data PCA: Scree Plot', fontsize=13)
ax.legend()
ax.grid(axis='y', alpha=0.3)
ax.set_xticks(np.arange(1, n_components_to_plot + 1))

# Right: Cumulative variance
ax = axes[1]
ax.plot(np.arange(n_components_to_plot) + 1, cumulative_variance[:n_components_to_plot] * 100, 'o-', linewidth=2)
ax.axhline(90, color='red', linestyle='--', label='90% threshold')
ax.axvline(n_90, color='green', linestyle='--', label=f'{n_90} components (90%)')
ax.set_xlabel('Number of Components', fontsize=12)
ax.set_ylabel('Cumulative Explained Variance (%)', fontsize=12)
ax.set_title('Raw Data PCA: Cumulative Variance', fontsize=13)
ax.legend()
ax.grid(alpha=0.3)
ax.set_xticks(np.arange(1, n_components_to_plot + 1))

plt.tight_layout()
plt.savefig(OUTPUT_DIR / 'raw_data_pca.png', dpi=150, bbox_inches='tight')
print(f"Saved: {OUTPUT_DIR / 'raw_data_pca.png'}")

# Visualization 2: Comparison with VAE latent space
fig, ax = plt.subplots(figsize=(10, 6))

# Plot raw data PCA
n_plot = min(12, len(explained_variance_ratio))
x = np.arange(1, n_plot + 1)
ax.bar(x - 0.2, explained_variance_ratio[:n_plot] * 100, width=0.4, alpha=0.7, label='Raw Data PCA', color='blue')

# Plot VAE latent PCA (from previous analysis)
vae_pc_variance = np.array([68.05, 15.78, 9.63, 3.07, 1.63, 0.83, 0.45, 0.24, 0.14, 0.09, 0.05, 0.03])
ax.bar(x + 0.2, vae_pc_variance, width=0.4, alpha=0.7, label='VAE Latent PCA (V2)', color='orange')

ax.axhline(10, color='red', linestyle='--', linewidth=1, alpha=0.5, label='10% threshold')
ax.set_xlabel('Principal Component', fontsize=12)
ax.set_ylabel('Explained Variance (%)', fontsize=12)
ax.set_title('Raw Data vs VAE Latent Space: PC Variance Comparison', fontsize=13)
ax.legend()
ax.grid(axis='y', alpha=0.3)
ax.set_xticks(x)

plt.tight_layout()
plt.savefig(OUTPUT_DIR / 'raw_vs_vae_pca_comparison.png', dpi=150, bbox_inches='tight')
print(f"Saved: {OUTPUT_DIR / 'raw_vs_vae_pca_comparison.png'}")

print()
print("=" * 80)
print("ANALYSIS COMPLETE")
print("=" * 80)
