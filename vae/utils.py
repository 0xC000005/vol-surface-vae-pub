import random, time, os, json
import numpy as np
import torch
import torch.optim as opt
from torch.utils.data import DataLoader
from tqdm import tqdm
from vae.base import BaseVAE
from vae.cvae_with_mem import CVAEMem
from vae.cvae import CVAE
import matplotlib.pyplot as plt
from matplotlib import cm
from collections import defaultdict
from typing import Union

def set_seeds(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

# perform model evaluation in terms of the accuracy and f1 score.
def model_eval(model: BaseVAE, dataloader):
    model.eval() # switch to eval model, will turn off randomness like dropout
    eval_losses = defaultdict(float)
    num_batches = 0
    for step, batch in enumerate(tqdm(dataloader, desc=f"eval")):
        try:
            batch.to(model.device)
        except:
            pass

        losses = model.test_step(batch)

        for k, v in losses.items():
            eval_losses[k] += v.item()
        num_batches += 1

    for k, v in eval_losses.items():
        eval_losses[k] = v / num_batches

    return eval_losses
    

def train(model: BaseVAE, train_dataloader: DataLoader, valid_dataloader: DataLoader, 
          lr=1e-5, epochs=100, 
          model_dir="./", file_name="vanilla.pt"):
    model.train()
    optimizer = opt.AdamW(model.parameters(), lr)
    best_dev_loss = np.inf

    ## run for the specified number of epochs
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    if "." in file_name:
        file_prefix = file_name.split(".")[0]
    else:
        file_prefix = file_name
    log_file = open(f"{model_dir}/{file_prefix}-{epochs}-log.txt", "w", encoding="utf-8")

    print("Model config: ", file=log_file)
    print(json.dumps(model.config, indent=True), file=log_file)
    print(f"LR: {lr}", file=log_file)
    print(f"Epochs: {epochs}", file=log_file)
    print("", file=log_file)
    log_file.flush()
    start_time = time.time()
    for epoch in range(epochs):
        epoch_start_time = time.time()
        model.train()
        train_losses = defaultdict(float)
        num_batches = 0
        for step, batch in enumerate(tqdm(train_dataloader, desc=f"train-{epoch}")):
            try:
                batch.to(model.device)
            except:
                pass

            losses = model.train_step(batch, optimizer)

            for k, v in losses.items():
                train_losses[k] += v.item()
            num_batches += 1
        for k, v in train_losses.items():
            train_losses[k] = v / (num_batches)
        
        dev_losses = model_eval(model, valid_dataloader)

        if dev_losses["loss"] < best_dev_loss:
            best_dev_loss = dev_losses["loss"]
            model.save_weights(optimizer, model_dir, file_prefix)

        formatted_train_loss = ", ".join([f'{k}: {v:.3f}' for k, v in train_losses.items()])
        formatted_dev_loss = ", ".join([f'{k}: {v:.3f}' for k, v in dev_losses.items()])
        print(f"epoch {epoch}: \ntrain loss :: {formatted_train_loss}, \ndev loss :: {formatted_dev_loss}, \ntime elapsed :: {time.time() - epoch_start_time}")
        print(f"epoch {epoch}: \ntrain loss :: {formatted_train_loss}, \ndev loss :: {formatted_dev_loss}, \ntime elapsed :: {time.time() - epoch_start_time}", file=log_file)
    print(f"training finished, total time :: {time.time() - start_time}")
    print(f"training finished, total time :: {time.time() - start_time}", file=log_file)
    log_file.close()
    return train_losses, dev_losses

def train_with_scheduled_sampling(
    model: BaseVAE,
    train_dataloader: DataLoader,
    valid_dataloader: DataLoader,
    lr=1e-5,
    epochs=400,
    model_dir="./",
    file_name="backfill_model.pt",
    teacher_forcing_epochs=200,
    horizons=[1, 7, 14, 30]
):
    """
    Two-phase training with scheduled sampling for backfilling.

    Phase 1 (epochs 0 to teacher_forcing_epochs):
        - Standard single-step teacher forcing using train_step()
        - Model learns to predict 1 day ahead with ground truth context
        - Builds strong short-term prediction capability

    Phase 2 (epochs teacher_forcing_epochs+1 to epochs):
        - Multi-horizon training using train_step_multihorizon()
        - Model learns to predict multiple horizons [1, 7, 14, 30] simultaneously
        - Improves long-term forecasting without forgetting short-term

    This approach follows the BACKFILL_MVP_PLAN.md Phase 2.2 design:
    - Gradual transition from teacher forcing to multi-horizon
    - Prevents catastrophic forgetting of short-term predictions
    - Improves stability compared to multi-horizon from scratch

    Args:
        model: CVAEMemRand instance (must have train_step_multihorizon method)
        train_dataloader: Training data (must have T >= 5 + 30 = 35 for max horizon)
        valid_dataloader: Validation data
        lr: Learning rate (default 1e-5)
        epochs: Total training epochs (default 400)
        model_dir: Directory to save model checkpoints
        file_name: Model filename (will add .pt extension)
        teacher_forcing_epochs: When to switch to multi-horizon (default 200)
        horizons: List of horizons for Phase 2 (default [1, 7, 14, 30])

    Returns:
        Tuple of (train_losses, dev_losses) from last epoch
    """
    model.train()
    optimizer = opt.AdamW(model.parameters(), lr)
    best_dev_loss = np.inf

    # Setup logging
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    if "." in file_name:
        file_prefix = file_name.split(".")[0]
    else:
        file_prefix = file_name
    log_file = open(f"{model_dir}/{file_prefix}-{epochs}-log.txt", "w", encoding="utf-8")

    print("=" * 80, file=log_file)
    print("SCHEDULED SAMPLING TRAINING", file=log_file)
    print("=" * 80, file=log_file)
    print("Model config: ", file=log_file)
    print(json.dumps(model.config, indent=True), file=log_file)
    print(f"LR: {lr}", file=log_file)
    print(f"Total Epochs: {epochs}", file=log_file)
    print(f"Teacher Forcing Epochs: {teacher_forcing_epochs} (Phase 1)", file=log_file)
    print(f"Multi-Horizon Epochs: {epochs - teacher_forcing_epochs} (Phase 2)", file=log_file)
    print(f"Multi-Horizon Targets: {horizons}", file=log_file)
    print("=" * 80, file=log_file)
    print("", file=log_file)
    log_file.flush()

    start_time = time.time()

    for epoch in range(epochs):
        epoch_start_time = time.time()
        model.train()
        train_losses = defaultdict(float)
        num_batches = 0

        # Determine training mode for this epoch
        if epoch < teacher_forcing_epochs:
            # Phase 1: Teacher Forcing (single-step)
            mode_str = f"Phase 1 - Teacher Forcing (epoch {epoch+1}/{teacher_forcing_epochs})"
            print(f"\n{mode_str}")

            for step, batch in enumerate(tqdm(train_dataloader, desc=f"train-{epoch}")):
                try:
                    batch.to(model.device)
                except:
                    pass

                # Standard single-step training
                losses = model.train_step(batch, optimizer)

                for k, v in losses.items():
                    train_losses[k] += v.item()
                num_batches += 1

        else:
            # Phase 2: Multi-Horizon Training
            mode_str = f"Phase 2 - Multi-Horizon {horizons} (epoch {epoch+1-teacher_forcing_epochs}/{epochs-teacher_forcing_epochs})"
            print(f"\n{mode_str}")

            for step, batch in enumerate(tqdm(train_dataloader, desc=f"train-{epoch}")):
                try:
                    batch.to(model.device)
                except:
                    pass

                # Multi-horizon training
                losses = model.train_step_multihorizon(batch, optimizer, horizons=horizons)

                for k, v in losses.items():
                    if isinstance(v, dict):
                        # horizon_losses is a dict, log each separately
                        for h, loss_val in v.items():
                            train_losses[f"{k}_h{h}"] += loss_val
                    else:
                        train_losses[k] += v if isinstance(v, float) else v.item()
                num_batches += 1

        # Average training losses
        for k, v in train_losses.items():
            train_losses[k] = v / num_batches

        # Validation (use single-step for consistency)
        dev_losses = model_eval(model, valid_dataloader)

        # Save best model based on validation loss
        if dev_losses["loss"] < best_dev_loss:
            best_dev_loss = dev_losses["loss"]
            model.save_weights(optimizer, model_dir, file_prefix)
            save_marker = "âœ“ BEST"
        else:
            save_marker = ""

        # Format and print losses
        formatted_train_loss = ", ".join([f'{k}: {v:.3f}' for k, v in train_losses.items()])
        formatted_dev_loss = ", ".join([f'{k}: {v:.3f}' for k, v in dev_losses.items()])
        epoch_time = time.time() - epoch_start_time

        print(f"{mode_str}")
        print(f"train loss :: {formatted_train_loss}")
        print(f"dev loss :: {formatted_dev_loss}")
        print(f"time :: {epoch_time:.1f}s {save_marker}")

        print(f"epoch {epoch}: \ntrain loss :: {formatted_train_loss}, \ndev loss :: {formatted_dev_loss}, \ntime elapsed :: {epoch_time} {save_marker}", file=log_file)
        log_file.flush()

    total_time = time.time() - start_time
    print(f"\n{'=' * 80}")
    print(f"Training finished! Total time: {total_time:.1f}s ({total_time/60:.1f} min)")
    print(f"Best validation loss: {best_dev_loss:.6f}")
    print(f"{'=' * 80}")

    print(f"\ntraining finished, total time :: {total_time}", file=log_file)
    print(f"best validation loss :: {best_dev_loss}", file=log_file)
    log_file.close()

    return train_losses, dev_losses

def test(model: BaseVAE, valid_dataloader: DataLoader, test_dataloader: DataLoader, model_fn="./vanilla"):
    model.load_weights(f=model_fn)
    dev_losses = model_eval(model, valid_dataloader)
    test_losses = model_eval(model, test_dataloader)

    formatted_dev_loss = ", ".join([f'{k}: {v:.3f}' for k, v in dev_losses.items()])
    formatted_test_loss = ", ".join([f'{k}: {v:.3f}' for k, v in test_losses.items()])
    print(f"dev loss: {formatted_dev_loss}, \ntest_loss: {formatted_test_loss}")
    return dev_losses, test_losses

def plot_surface(original_data: np.ndarray, vae_output: np.ndarray):
    fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
    x = [0.7,0.85,1,1.15,1.3]
    y = [0.08333,0.25,0.5,1,2]
    x, y = np.meshgrid(x, y)

    ax.plot_surface(x, y, original_data, cmap=cm.coolwarm, linewidth=0, antialiased=False)
    ax.plot_surface(x, y, vae_output, cmap=cm.coolwarm, linewidth=0, antialiased=False)
    ax.set_xlabel("K/S")
    ax.set_ylabel("ttm")
    plt.show()

def plot_surface_separate(original_data: np.ndarray, vae_output: np.ndarray):
    fig, ax = plt.subplots(nrows=1, ncols=3, subplot_kw={"projection": "3d"})
    x = [0.7,0.85,1,1.15,1.3]
    y = [0.08333,0.25,0.5,1,2]
    x, y = np.meshgrid(x, y)

    ax[0].plot_surface(x, y, original_data, cmap=cm.coolwarm, linewidth=0, antialiased=False)
    ax[0].set_xlabel("K/S")
    ax[0].set_ylabel("ttm")
    ax[0].set_title("Original surface")
    
    ax[1].plot_surface(x, y, vae_output, cmap=cm.coolwarm, linewidth=0, antialiased=False)
    ax[1].set_xlabel("K/S")
    ax[1].set_ylabel("ttm")
    ax[1].set_title("VAE surface")
    
    ax[2].plot_surface(x, y, original_data, cmap=cm.Blues, linewidth=0, antialiased=False)
    ax[2].plot_surface(x, y, vae_output, cmap=cm.Reds, linewidth=0, antialiased=False)
    ax[2].set_xlabel("K/S")
    ax[2].set_ylabel("ttm")
    ax[2].set_title("Both surfaces")
    
    plt.subplots_adjust(right=2.0, wspace=0.3)
    plt.show()

def generate_surface_path(surf_data, ex_data, model_data, path_idx=8000, model_type: Union[CVAE, CVAEMem] = CVAEMem):
    '''
        This function is for SABR data. Also this function applies an aggregated approach. 
        i.e. generate new surfaces based on previously generated data
    '''
    model_config = model_data["model_config"]
    model = model_type(model_config)
    model.load_weights(dict_to_load=model_data)
    seq_len = model_config["seq_len"]
    if "ex_feats_dim" in model_config:
        use_ex_feats = model_config["ex_feats_dim"] > 0
    else:
        use_ex_feats = False
    all_simulation = {
        "surface": [surf_data[path_idx][i] for i in range(seq_len-1)],
        "ex_feats": [ex_data[path_idx][i] for i in range(seq_len-1)] if use_ex_feats else None,
    }
    steps_to_sim = len(surf_data[path_idx]) + 1 - seq_len
    for i in range(steps_to_sim):
        ctx_data = {
            "surface": torch.from_numpy(np.array(all_simulation["surface"][i:(i+seq_len-1)])), 
            "ex_feats": torch.from_numpy(np.array(all_simulation["ex_feats"][i:(i+seq_len-1)])).unsqueeze(-1) if use_ex_feats else None
        }
        if use_ex_feats:
            surf, ex_feat = model.get_surface_given_conditions(ctx_data) 
        else:
            ctx_data.pop("ex_feats")
            surf = model.get_surface_given_conditions(ctx_data) 
        surf = surf.detach().cpu().numpy().reshape((5,5))
        all_simulation["surface"].append(surf)
        if use_ex_feats:
            ex_feat = ex_feat.detach().cpu().numpy().reshape((1,))[0]
            all_simulation["ex_feats"].append(ex_feat)

    all_simulation["surface"] = np.array(all_simulation["surface"])
    if use_ex_feats:
        all_simulation["ex_feats"] = np.array(all_simulation["ex_feats"])
    
    return all_simulation

def generate_surface_spx(surf_data, ex_data, model_data, start_time=5000, steps_to_sim=30, model_type: Union[CVAE, CVAEMem] = CVAEMem):
    '''
        This function is for S&P500 data. Also this function applies an aggregated approach. 
        i.e. generate new surfaces based on previously generated data

        This function uses the original return/price as ex features for simulation
    '''
    model_config = model_data["model_config"]
    model = model_type(model_config)
    model.load_weights(dict_to_load=model_data)
    seq_len = model_config["seq_len"]
    ctx_len = model_config["ctx_len"]
    if "ex_feats_dim" in model_config:
        use_ex_feats = model_config["ex_feats_dim"] > 0
    else:
        use_ex_feats = False
    all_simulation = {
        "surface": [surf_data[start_time+i] for i in range(ctx_len)],
        "ex_feats": [ex_data[start_time+i] for i in range(steps_to_sim + ctx_len)] if use_ex_feats else None,
    }
    for i in range(steps_to_sim):
        ctx_data = {
            "surface": torch.from_numpy(np.array(all_simulation["surface"][i:(i+ctx_len)])), 
            "ex_feats": torch.from_numpy(np.array(all_simulation["ex_feats"][i:(i+ctx_len)])).unsqueeze(-1) if use_ex_feats else None
        }
        if use_ex_feats:
            surf, _ = model.get_surface_given_conditions(ctx_data) 
        else:
            ctx_data.pop("ex_feats")
            surf = model.get_surface_given_conditions(ctx_data) 
        surf = surf.detach().cpu().numpy().reshape((5,5))
        all_simulation["surface"].append(surf)
        # if use_ex_feats:
        #     ex_feat = ex_feat.detach().cpu().numpy().reshape((1,))[0]
        #     all_simulation["ex_feats"].append(ex_feat)

    all_simulation["surface"] = np.array(all_simulation["surface"])
    if use_ex_feats:
        all_simulation["ex_feats"] = np.array(all_simulation["ex_feats"])
    
    return all_simulation

def plot_surface_time_series(vae_output, title_label="VAE"):
    if isinstance(vae_output, dict):
        surfaces = vae_output["surface"]
        ex_feats = vae_output["ex_feats"]
    else:
        surfaces = vae_output
        ex_feats = None
    nrows = surfaces.shape[0] // 5
    if surfaces.shape[0] % 5 != 0:
        nrows += 1
    fig, ax = plt.subplots(nrows=nrows, ncols=5, subplot_kw={"projection": "3d"})
    x = [0.7,0.85,1,1.15,1.3]
    y = [0.08333,0.25,0.5,1,2]
    x, y = np.meshgrid(x, y)

    for i in range(len(surfaces)):
        r = i // 5
        c = i % 5
        ax[r][c].plot_surface(x, y, surfaces[i], cmap=cm.coolwarm, linewidth=0, antialiased=False)
        ax[r][c].set_xlabel("K/S")
        ax[r][c].set_ylabel("ttm")
        if ex_feats is not None:
            ax[r][c].set_title(f"{title_label} surface on day {i},\nex_feat={ex_feats[i]:.4f}")
        else:
            ax[r][c].set_title(f"{title_label} surface on day {i}")
    
    plt.subplots_adjust(right=4.0, top=4.0, hspace=0.5)
    plt.show()